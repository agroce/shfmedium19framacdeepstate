The core problem we aim to address in this proposal is that \emph{use of formal modeling, advanced static analysis, and advanced dynamic analysis tools in C and C++} for verification and validation, especially on critical, timing-dependent, embedded and cyber-physical systems, is prohibitively difficult and lacks sufficient synergy for effective application in real-world projects.  This limitation applies even to systems built in an academic research context, unless the context is specifically that of using such systems (that is, unless the research is about methods for verifying systems, not just about the systems themselves).  Furthermore, even when use of these techniques to ensure correctness, reliability, or security \emph{is} a focus of the project, such use is almost always limited to one type of effort --- modeling, static analysis (including proof of correctness for limited components of a system), or dynamic analysis (including automated test case generation).  At heart, we believe the cause for this difficulty is the \emph{lack of synergy} between these related efforts, the failure of effort in one context to transfer to another context.  In short:

\begin{itemize}
\item Learning to use a formal modeling language and tool, such as \uppaal~\cite{uppaal}, \prism~\cite{KNP2011:CAV}, or \spin~\cite{SPIN}, provides help in discovering defects in a high-level, abstract formulation of a model or protocol, but usually does not translate, in any way, into assistance with implementation-level problems that are not directly modeled in the formalism.
\item Many static analysis tools are primarily ``bug detectors'' (e.g, Coverity or CodeSonar), whose output is essentially limited to a list of possible problems.  These tools, however, seldom provide any means for producing tests that can help distinguish false positives from real problems with the code.
\item More powerful static analysis tools, such as \framac~\cite{KKP2015:FAC}, provide proofs of correctness for limited aspects of a system, and a rich specification and annotation language.  Yet, again, there is no connection between this annotation and either formal modeling or most test generation tools.  Use of dynamic analysis with \framac is limited to its own concolic execution tool, and lacks support for other generation methods such as fuzzing.
\item There are a large variety of automated test generation tools; however, again, effort spent in learning one of these tools only partially applies to learning a different tool.  Furthermore, many of the most powerful such tools (e.g., AFL ~\cite{aflfuzz}) are specialized to the problem of generating files or packets for use in security vulnerability detection, and provide no support for  the kind of testing needed to detect and understand other types of faults in e.g., communication protocols used in distributed embedded systems.  Finally, none of these tools significantly leverage specification and verification effort from formal modeling or advanced static analysis, or even knowledge gained in these efforts.
\end{itemize}

Consider the case of an engineer working on a custom, low-energy consumption, communication protocol for use in a distributed system consisting of low-power sensors and actuators.  If the engineer builds a formal model of the protocol, she will discover that this extensive effort provides no help, other than an improved concept of the system, in proving the correctness of part of the actual implementation, even if the property to be proved has a representation in the model.  If the engineer begins instead by building a test generation harness, using an automated test generation tool, she again discovers that despite having spent considerable time expressing pre-conditions and post-conditions for various functions in the implementation, in order to guide test generation, this work must be duplicated when she decides to try to formally prove the correctness of certain core functionality.  Had she begun with the proofs, again, the logically related (or even equivalent) information about the code would have to be re-expressed, in a different language, in order to proceed with test case generation.  Not only must our engineer learn three tools, but effort spent in using one tool essentially never carries over to another part of the effort.  In almost all cases, there is simply not enough time or energy available to make use of the full spectrum of available technology for avoiding defects and ensuring reliability of the system.  One approach may be selected, if an engineer is familiar with that method, or, in practice, no advanced correctness technology may be used at all.  After all, it is hard to predict whether formal modeling, static analysis, or dynamic analysis will have the greatest payoff, or even work well, and so perhaps it is best to just put somewhat more effort into scenario design and manual testing.

\subsection{Proposed Solution}

While allowing efforts from any form of formal or automated verification or validation attempt to maximally carry over to other forms (i.e., formal models to code annotations for static analysis, code annotations to test harnesses, test harnesses to code annotations, test harnesses to formal models, formal models to test harnesses, and code annotations to formal models) is the ideal goal, simply making it possible to follow \emph{one} critical path to combine methods is feasible given current technologies in the sub-domains (formal modeling, static analysis, and dynamic analysis) and a set of specific advances in bridging the gap between the technologies.  This project proposes to make it possible 1) to transfer efforts to build a formal protocol or system model using timed automata into code annotations making static analysis and proof of implementation correctness possible and 2) to transfer specification and verification effort from advanced static analysis tools to the automated generation of tests using a variety of advanced tools, including symbolic execution tools and highly effective gray-box fuzzers.  Enabling this path also enables, indirectly, the use of automated test generation tools on the implementation of a system represented by a timed automata model, even if no effort is made to prove the implementation correct.  To further improve the value of our approach, we focus on integrating static and dynamic analysis tools that are, themselves, frameworks/front-ends allowing application of multiple approaches within those domains.  This project is specifically addressed to communication protocol implementations in embedded systems where timing is critical to the modeling of behavior, but we expect that our solution will generalize to other critical C and C++ systems development scenarios.  Additionally, we target the common, hard, case where our approach may be used to guide creation of new code, but will be applied mostly to systems with partial or complete (but not sufficiently reliable and verified) implementations, in typical embedded C code.  We expect developers to learn new tools, but not new programming paradigms or even languages.

\subsubsection{PI Qualifications}

PI Groce has a long history with formal methods, including involvement in design and development of well-known model checkers, and application of model checkers at NASA/JPL on flight software for the Mars rovers.  More recently, he has primarily focused on developing algorithms and tools for automated software testing, including front-ends allowing a user to specify a testing problem once and use multiple back-ends; he is a core member of the DeepState~\cite{DeepState,deepstatetutorial,deepstaterepo} design and development team.  He brings to this project practical experience using advanced verification and testing tools on real systems, and engineering such tools to be useable by engineers who are domain experts, not verification or testing experts.

Co-PI Loulergue has a long experience in designing parallel programming languages and libraries based on formal semantics. About ten years ago, he started using the \Coq proof assistant in his research on programming language semantics and on the development of parallel programs correct by construction. More recently he has begun a collaboration with colleagues at {\it Commissariat \`a l'\'energie atomique et aux \'energies alternatives} (CEA). This line of work focuses on extending \framac with new features, always starting from needs coming from analysis and verification tasks on real-world code. He brings to this project a strong expertise on static analysis and deductive verification of C programs with the \framac framework, and on specification and proof engineering on real-world code. 

Co-PI Nghiem has an extensive background in control theory and application of formal methods in control systems.
He has long experience working with timed automata and the \uppaal tool family.
He developed methods for testing and verifying temporal logic specifications of hybrid systems -- systems that exhibit both continuous and discrete behaviors -- and was granted a U.S. patent for his methods.
He brings to this project advanced knowledge about real-time embedded systems and practical experience applying verification methods and tools on those systems.

Co-PI Flikkema's current work includes research in energy-efficient embedded systems and networks, inference of the embedding environment, wireless sensor/actuator networks for monitoring and control of environmental and ecological systems, and cybersecurity with focus on IoT.  Like Co-PI Nghiem, he ensures that developed approaches will be suitable for engineers whose primary focus is \emph{building working systems}, not verification or testing theory.