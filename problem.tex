The core problem we aim to address in this proposal is that \emph{use of formal modeling, advanced static analysis, and advanced dynamic analysis tools in C and C++} for verification and validation, especially on critical, timing-dependent, embedded and cyber-physical systems, is prohibitively difficult and lacks sufficient synergy for effective application in real-world projects.  This limitation applies even to systems built in an academic research context, unless the context is specifically that of using such systems (that is, unless the research is primarily \emph{about} methods for verifying systems).  Furthermore, even when use of these techniques to ensure correctness, reliability, or security \emph{is} a focus of the project, such use is almost always limited to one type of effort---modeling, static analysis (including proof of correctness for components of a system), or dynamic analysis (e.g., automated test case generation).  A major cause for this difficulty is the \emph{lack of synergy} between these related efforts, the failure of effort in one context to transfer to another context.  In short:

\begin{itemize}[labelsep=3pt,leftmargin=12pt]
\item Learning to use a formal modeling language and tool, such as \uppaal~\cite{uppaal}, \prism~\cite{KNP2011:CAV}, or \spin~\cite{SPIN}, provides help in discovering defects in a high-level, abstract formulation of a model or protocol, but seldom helps with implementation-level problems not directly modeled in the formalism.
\item Many static analysis tools are primarily ``bug detectors'' (e.g, Coverity or CodeSonar), whose output is essentially limited to a list of possible problems.  These tools, however, seldom provide any means for producing tests that can help distinguish false positives from real problems with the code.
\item More powerful static analysis tools, such as \framac~\cite{KKP2015:FAC}, provide proofs of correctness for limited aspects of a system, and a rich specification and annotation language.  Yet, again, there is no connection between this annotation and either formal modeling or most test generation tools.  Use of dynamic analysis with \framac is limited to its own concolic execution tool.
\item There are a large variety of automated test generation tools; however, again, effort spent in learning one of these tools only partially applies to learning a different tool.  Furthermore, many of the most powerful such tools (e.g., AFL ~\cite{aflfuzz}) are specialized to the problem of finding memory safety vulnerabilities, and provide no support for  the kind of testing needed for  other types of faults in e.g., communication protocols used in distributed embedded systems.  Finally, none of these tools significantly leverage specification and verification effort from formal modeling or advanced static analysis.
\end{itemize}

Consider the case of an engineer working on a custom, low-energy consumption, communication protocol for use in a distributed system consisting of low-power sensors and actuators.  If the engineer builds a formal model of the protocol, she will discover that this extensive effort provides no help, other than an improved concept of the system, in proving the correctness of the actual implementation, even if the property to be proved exists in the model.  If the engineer begins instead by building an automated test generation harness she again discovers that despite having spent considerable time expressing pre-conditions and post-conditions for various functions in the implementation, to guide test generation, the work must be duplicated when she decides to try to formally prove the correctness of core functionality.  Had she begun with the proofs, again, logically related (or even equivalent) information would have to be re-expressed, in a different language, to perform test generation.  Not only must our engineer learn three tools, but effort spent in using one tool almost never carries over to another approach.  In almost all cases, there is simply not enough time or energy available to make use of the full spectrum of available technology.  In practice, \emph{no advanced correctness technology may be used at all.}  After all, it is hard to predict which technology will have the greatest payoff, or even work at all, so perhaps it is best to just put more effort into manual testing.

\subsection{Proposed Solution}

While allowing efforts from any form of formal or automated verification or validation attempt to maximally carry over to other forms (i.e., formal models to code annotations for static analysis, code annotations to test harnesses, test harnesses to code annotations, test harnesses to formal models, formal models to test harnesses, and code annotations to formal models) is the ideal goal, simply making it possible to follow \emph{one} critical path to combine methods is feasible given current technologies in the sub-domains (formal modeling, static analysis, and dynamic analysis) and a set of specific advances in bridging the gap between the technologies.  This project proposes to make it possible 1) to transfer efforts to build a formal protocol or system model using timed automata into code annotations for static analysis and proof of implementation correctness and 2) to transfer specification and verification effort from this static analysis to automated generation of tests using symbolic execution tools and gray-box fuzzers.  Enabling this path also enables the use of automated test generation tools with the specification provided by a timed automata model.  To further improve the value of our approach, we focus on integrating static and dynamic analysis tools that are, themselves, frameworks/front-ends allowing application of multiple approaches.  This project is specifically focused on communication protocol implementations in embedded systems where timing is critical to the modeling of behavior, but we expect that our solution will generalize to other critical C and C++ systems development scenarios.  Additionally, we target the common, hard, case where our approach will be applied to systems with partial or complete implementations: typical legacy embedded C code.  We expect developers to learn new tools, but not new programming paradigms or languages.

\subsubsection{PI Qualifications}

PI Groce has a long history with formal methods, including involvement in design and development of well-known model checkers, and application of model checkers at NASA/JPL on flight software for the Mars rovers.  More recently, he has primarily focused on developing algorithms and tools for automated software testing, including front-ends allowing a user to specify a testing problem once and use multiple back-ends; he is a core member of the DeepState~\cite{DeepState,deepstatetutorial,deepstaterepo} design and development team.  He brings to this project practical experience using advanced verification and testing tools on real systems, and engineering such tools to be useable by engineers who are domain experts, not verification or testing experts.

Co-PI Loulergue has a long experience in designing parallel programming languages and libraries based on formal semantics. About ten years ago, he started using the \Coq proof assistant in his research on programming language semantics and on the development of parallel programs correct by construction. More recently he has begun a collaboration with colleagues at {\it Commissariat \`a l'\'energie atomique et aux \'energies alternatives} (CEA). This line of work focuses on extending \framac with new features, always starting from needs coming from analysis and verification tasks on real-world code. He brings to this project a strong expertise on static analysis and deductive verification of C programs with the \framac framework, and on specification and proof engineering on real-world code. 

Co-PI Nghiem has an extensive background in control and autonomous systems, and application of formal methods in control systems.
He has long experience working with timed automata and the \uppaal tool family.
He developed methods for testing and verifying temporal logic specifications of hybrid systems -- systems that exhibit both continuous and discrete behaviors -- and was granted a U.S. patent for his methods.
He brings to this project advanced knowledge about real-time embedded systems and practical experience applying verification methods and tools on those systems.
He will also lead a case study that applies the methods and tools developed in this project to a multi-agent robotics system.

Co-PI Flikkema's current work includes research in energy-efficient embedded systems and networks, inference of the embedding environment, wireless sensor/actuator networks for monitoring and control of environmental and ecological systems, and cybersecurity with focus on IoT.  Like Co-PI Nghiem, he ensures that developed approaches will be suitable for engineers whose primary focus is \emph{building working systems}.