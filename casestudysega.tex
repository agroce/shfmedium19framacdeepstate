The case study informing this research is the embedded software used in large collection of operational wireless sensor/actuator networks for monitoring and control of ecological systems~\cite{ClaEtAl11,GhoEtAl2014,BelEtAl2015}. The nodes use a multi-processor architecture: a central processor provides services, including scheduling and dispatch of tasks, storage, and a message-passing interface for wireless networking. Plug-in satellite processors handle transducer sampling, actuation, and related computational tasks. In addition to allowing true parallelism, this architecture enables hardware-level improvements in energy efficiency, since each satellite can be optimized for its specific task. More practically, it admits the rapid implementation of highly heterogenous nodes that incorporate a wide range of sensing, actuation, and sensing+actuation
capabilities. Our implementation of the architecture emphasizes energy efficiency~\cite{FliSENSORS2010,FliICC2011}.  For example, all satellite processors are power-gated via central processor control; ensuring that satellite processors are depowered prevents satellite sleep-mode energy leakage. The power subsystem provides multiple power buses at different voltages, including an optically-isolated high-power bus for actuation.  A variety of energy supplies are also supported, including battery-backed photovoltaic sources~\cite{FliEtAl12,KnaFli17}.

The nodes synchronously interact with neighbors in a multi-hop, self-organizing/healing network; communication synchronization is implemented as scheduled rendezvous in time slots; slot boundaries are in turn managed by a lightweight global time synchronization protocol that is integrated with low-level communication synchronization.  This approach, implemented using a time-triggered architecture, minimizes communication energy cost, which dominates the overall energy consumption in these applications. Because timing is critical and is determined by the embedded system hardware and software, most testing has occurred at the network level, with extensive in-lab testing with small networks and instrumented field tests.  However, we have found in our long-term deployments (at dozens of field sites over years of operation) that very occasionally the networking fails and nodes become isolated---we think due to a complex set of subtle bugs rooted in different levels of timing abstraction.  

Because access to SEGA installations can be difficult, and in the long run many may be located so remotely that it is cost-prohibitive to send humans to address problems, discovering the source of these in-operation faults, identifying other faults, and generally improving the reliability of the system.  We therefore aim to use SEGA (and in particular the protocol in question and its implementation) as a case study for our methods.  This will enable us to apply our approach in a practical setting, and increase the chances that what we produce is actually usable by engineers of real systems.

First, we will model the protocol itself as a timed automata in \uppaal or \prism, in order to ensure that there is not a subtle flaw in the protocol itself, and to model our expectations of behavior in the real system.  Then, following our proposed workflow, we will automatically annotate the implementation with specification extracted from the specification of the timed automata model, and attempt to prove components of the code faithfully represent the intended behavior.  Either of these steps may, of course, expose the source of the mysterious networking failures.  Whether at this point the current problem is identified or not, we will finally use DeepState, driven by harnesses automatically generated by our tools, to generate tests of the implementation components in question.  Even if (as we believe is unlikely) \framac is able to prove most individual functions ``correct'' the DeepState testing may expose faults that are not part of the specification.  For instance, using libFuzzer with DeepState we can use LLVM's Undefined Behavior sanitized to catch some classes of undefined behavior that \framac does not take into account.  Furthermore, \framac's ability to prove properties about interactions of multiple functions operating in arbitrary sequence is often limited; such proofs are notoriously hard to construct in general.  DeepState allows us to hope to detect faults when we cannot prove correctness.  DeepState's ability to use symbolic execution as a back-end will be most useful for verifying single functions that are hard to verify with \framac, while state-of-the-art fuzzers will be most useful for sets of functions, or cases where symbolic execution fails to scale.

This system is ideal as a case study for several reasons. First, the deployed networks “fail” in way that enables exploring how to design, prove, and test time-critical systems in a way that does no harm: human life is not affected in this application, and data is not lost, since all sensed information is logged locally at each node as a back-up.  On the other hand, reliable operation is important; as noted above, access to manually fix problems can be problematic even with current installations, and in the future this problem will only grow.  Second, some failure modes could damage or even destroy (through e.g. over-watering) long-running scientific experiments.
Secondly, this application uses common data structures for task control blocks, and the operating system at each node schedules and dispatches both periodic and pseudo-randomly scheduled tasks. Thus the system is a good example of the highly general applications of scheduling and synchronization used in myriad time-critical systems.  The embedded code is written in C, enabling the use of both \framac and DeepState. More broadly, it will inform how the developed theory and tools can improve the correctness, reliability, and safety of cyber-physical and IoT applications.